{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7790d7d8",
   "metadata": {},
   "source": [
    "## 1 Downloading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading dataset to train on.\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4b019b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the dataset :  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset.txt','r',encoding='utf-8') as data:\n",
    "    text = data.read()\n",
    "    \n",
    "print(\"Total number of characters in the dataset : \",len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "55da87ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 characters :  First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print('First 100 characters : ', text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f0638",
   "metadata": {},
   "source": [
    "## 2 Processing The Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49679be",
   "metadata": {},
   "source": [
    "### 2.1 Building Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a282af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in the dataset = 65\n",
      "\n",
      "Which are following : \n",
      " \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print('Number of unique characters in the dataset =',vocab_size)\n",
    "print('\\nWhich are following : \\n',''.join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18925d9f",
   "metadata": {},
   "source": [
    "### 2.2 Building Tokenizer  (Custom Encoder, Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aa74b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping individual charaters to integers \n",
    "\n",
    "encoder = { char:i for i,char in enumerate(chars)}\n",
    "decoder = { i:char for i,char in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [encoder[char] for char in string]\n",
    "decode = lambda integers: [decoder[i] for i in integers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a974523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Zain Khalid \n",
      "=  [38, 39, 47, 52, 1, 23, 46, 39, 50, 47, 42]\n",
      "\n",
      "Decoded  [38, 39, 47, 52, 1, 23, 46, 39, 50, 47, 42] \n",
      "= Zain Khalid\n"
     ]
    }
   ],
   "source": [
    "zk = encode('Zain Khalid')\n",
    "print('Encoded Zain Khalid \\n= ',zk)\n",
    "print('\\nDecoded ',zk,'\\n=', ''.join(decode(zk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514db2d8",
   "metadata": {},
   "source": [
    "### 2.3 Encoding The Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d82e617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0d235dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "data = tf.convert_to_tensor(encode(text), dtype=tf.int64)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d9d64c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9ce5a",
   "metadata": {},
   "source": [
    "### 2.4 Splitting the dataset (Train,Validate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0657583",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:limit]\n",
    "val_data = data[limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "021ae46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f96263",
   "metadata": {},
   "source": [
    "### 2.5 Chunking Dataset in Blocks (x,y) (To Train Transformer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa008ed1",
   "metadata": {},
   "source": [
    "#### Concept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df0f103a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int64, numpy=array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int64)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e16ad2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for input:  [18]   target is:  47\n",
      "for input:  [18, 47]   target is:  56\n",
      "for input:  [18, 47, 56]   target is:  57\n",
      "for input:  [18, 47, 56, 57]   target is:  58\n",
      "for input:  [18, 47, 56, 57, 58]   target is:  1\n",
      "for input:  [18, 47, 56, 57, 58, 1]   target is:  15\n",
      "for input:  [18, 47, 56, 57, 58, 1, 15]   target is:  47\n",
      "for input:  [18, 47, 56, 57, 58, 1, 15, 47]   target is:  58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for token in range(block_size):\n",
    "    context = x[:token+1]\n",
    "    target = y[token]\n",
    "    print('for input: ',context.numpy().tolist(),'  target is: ',target.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a129388",
   "metadata": {},
   "source": [
    "#### Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "85bc31c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[63  1 61 43 39 49 52 43]\n",
      " [43  6  1 51 63  1 50 53]\n",
      " [ 1 58 46 43 51  1 44 59]\n",
      " [50 50  1 58 46 43 43  1]], shape=(4, 8), dtype=int64)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[ 1 61 43 39 49 52 43 57]\n",
      " [ 6  1 51 63  1 50 53 56]\n",
      " [58 46 43 51  1 44 59 56]\n",
      " [50  1 58 46 43 43  1 52]], shape=(4, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 #Number of independent input sequences to process in parallel for GPU\n",
    "block_size = 8 #Maximum context length to make predictions\n",
    "n_embd = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate small batches of input x & target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    randPos = tf.dtypes.cast(tf.random.uniform((batch_size,), minval=0, maxval=(len(data)-block_size)), dtype=tf.int32)\n",
    "    #print(randPos) # random positions in the whole datasets to grab block size chunks\n",
    "    xbatch = tf.stack([data[i:i+block_size] for i in randPos])\n",
    "    ybatch = tf.stack([data[i+1:i+block_size+1] for i in randPos])\n",
    "    \n",
    "    return xbatch, ybatch\n",
    "\n",
    "xbatch, ybatch = get_batch('train')\n",
    "\n",
    "print('inputs:')\n",
    "print(xbatch.shape)\n",
    "print(xbatch)\n",
    "\n",
    "print('targets:')\n",
    "print(ybatch.shape)\n",
    "print(ybatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55db59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for input:  [63]   target is:  1\n",
      "for input:  [63, 1]   target is:  61\n",
      "for input:  [63, 1, 61]   target is:  43\n",
      "for input:  [63, 1, 61, 43]   target is:  39\n",
      "for input:  [63, 1, 61, 43, 39]   target is:  49\n",
      "for input:  [63, 1, 61, 43, 39, 49]   target is:  52\n",
      "for input:  [63, 1, 61, 43, 39, 49, 52]   target is:  43\n",
      "for input:  [63, 1, 61, 43, 39, 49, 52, 43]   target is:  57\n",
      "for input:  [43]   target is:  6\n",
      "for input:  [43, 6]   target is:  1\n",
      "for input:  [43, 6, 1]   target is:  51\n",
      "for input:  [43, 6, 1, 51]   target is:  63\n",
      "for input:  [43, 6, 1, 51, 63]   target is:  1\n",
      "for input:  [43, 6, 1, 51, 63, 1]   target is:  50\n",
      "for input:  [43, 6, 1, 51, 63, 1, 50]   target is:  53\n",
      "for input:  [43, 6, 1, 51, 63, 1, 50, 53]   target is:  56\n",
      "for input:  [1]   target is:  58\n",
      "for input:  [1, 58]   target is:  46\n",
      "for input:  [1, 58, 46]   target is:  43\n",
      "for input:  [1, 58, 46, 43]   target is:  51\n",
      "for input:  [1, 58, 46, 43, 51]   target is:  1\n",
      "for input:  [1, 58, 46, 43, 51, 1]   target is:  44\n",
      "for input:  [1, 58, 46, 43, 51, 1, 44]   target is:  59\n",
      "for input:  [1, 58, 46, 43, 51, 1, 44, 59]   target is:  56\n",
      "for input:  [50]   target is:  50\n",
      "for input:  [50, 50]   target is:  1\n",
      "for input:  [50, 50, 1]   target is:  58\n",
      "for input:  [50, 50, 1, 58]   target is:  46\n",
      "for input:  [50, 50, 1, 58, 46]   target is:  43\n",
      "for input:  [50, 50, 1, 58, 46, 43]   target is:  43\n",
      "for input:  [50, 50, 1, 58, 46, 43, 43]   target is:  1\n",
      "for input:  [50, 50, 1, 58, 46, 43, 43, 1]   target is:  52\n"
     ]
    }
   ],
   "source": [
    "for row in range(batch_size):\n",
    "    for token in range(block_size):\n",
    "        context = xbatch[row, :token+1]\n",
    "        target = ybatch[row, token]\n",
    "        print('for input: ',context.numpy().tolist(),'  target is: ',target.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702b11d",
   "metadata": {},
   "source": [
    "## 3 Without Transformer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e218dca",
   "metadata": {},
   "source": [
    "## Maths for self-attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "392a170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want a method to preserve context for each word and it's former words in the time block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b40b6f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for input:  [63]   target is:  1\n",
      "for input:  [63, 1]   target is:  61\n",
      "for input:  [63, 1, 61]   target is:  43\n",
      "for input:  [63, 1, 61, 43]   target is:  39\n",
      "for input:  [63, 1, 61, 43, 39]   target is:  49\n",
      "for input:  [63, 1, 61, 43, 39, 49]   target is:  52\n",
      "for input:  [63, 1, 61, 43, 39, 49, 52]   target is:  43\n",
      "for input:  [63, 1, 61, 43, 39, 49, 52, 43]   target is:  57\n"
     ]
    }
   ],
   "source": [
    "# example of words and their context\n",
    "\n",
    "for row in range(batch_size-3):\n",
    "    for token in range(block_size):\n",
    "        context = xbatch[row, :token+1]\n",
    "        target = ybatch[row, token]\n",
    "        print('for input: ',context.numpy().tolist(),'  target is: ',target.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "17c0afc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbatch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed2be5",
   "metadata": {},
   "source": [
    "### 3.1 Aggregating for context (simplest method) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89887139",
   "metadata": {},
   "source": [
    "#### Version 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca46de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = tf.random.normal((B, T, C), dtype=tf.float64)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "415b407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = tf.zeros((B, T, C), dtype=tf.float64)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1, :]  # (t, C)\n",
    "        mean_value = tf.reduce_mean(xprev, axis=0)\n",
    "        xbow = tf.tensor_scatter_nd_add(xbow, indices=[[b, t]], updates=[mean_value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f46d2737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 2), dtype=float64, numpy=\n",
       "array([[ 0.95310103, -0.33585035],\n",
       "       [-0.89537386, -0.10109913],\n",
       "       [-1.08746957, -0.23392829],\n",
       "       [ 0.55400938, -3.05065275],\n",
       "       [-0.4705944 , -1.86886873],\n",
       "       [ 2.04610976, -0.4255422 ],\n",
       "       [-1.76064349,  0.43410932],\n",
       "       [-1.65781871,  0.70082522]])>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n",
    "#original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4a3870a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8, 2), dtype=float64, numpy=\n",
       "array([[[ 0.95310103, -0.33585035],\n",
       "        [ 0.02886359, -0.21847474],\n",
       "        [-0.34324746, -0.22362592],\n",
       "        [-0.11893325, -0.93038263],\n",
       "        [-0.18926548, -1.11807985],\n",
       "        [ 0.18329706, -1.00265691],\n",
       "        [-0.09440874, -0.79740459],\n",
       "        [-0.28983498, -0.61012586]],\n",
       "\n",
       "       [[ 0.24474915,  0.5304757 ],\n",
       "        [-0.0215186 ,  0.89519942],\n",
       "        [-0.29395758,  0.44847173],\n",
       "        [ 0.17669387,  0.5498084 ],\n",
       "        [ 0.18008692,  0.3448098 ],\n",
       "        [-0.0328576 ,  0.4573222 ],\n",
       "        [-0.09852322,  0.57093864],\n",
       "        [ 0.09555441,  0.39454034]],\n",
       "\n",
       "       [[-0.45030911, -0.85046188],\n",
       "        [ 0.61036958, -0.90442377],\n",
       "        [ 0.601554  , -1.25840561],\n",
       "        [ 0.25857243, -0.86086016],\n",
       "        [ 0.28360905, -0.79047013],\n",
       "        [ 0.316442  , -0.62021193],\n",
       "        [ 0.02984569, -0.50866335],\n",
       "        [-0.3375226 , -0.46764786]],\n",
       "\n",
       "       [[ 0.18904454, -1.09819457],\n",
       "        [ 0.18068477,  0.22778135],\n",
       "        [-0.22051126,  0.27002698],\n",
       "        [-0.40188967, -0.17778307],\n",
       "        [-0.46805949, -0.37343779],\n",
       "        [-0.67184519, -0.11559903],\n",
       "        [-0.50568571,  0.08460857],\n",
       "        [-0.35136245,  0.03698063]]])>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregated data\n",
    "xbow\n",
    "#each upcoming row is contains the aggregate of all previous "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e626e23",
   "metadata": {},
   "source": [
    "### 3.2 Efficient aggregating for context (simplest method) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2bd82",
   "metadata": {},
   "source": [
    "#### Concept (Same achived through matrix multiplication trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e34713f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "[[1.         0.         0.        ]\n",
      " [0.5        0.5        0.        ]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "--\n",
      "b=\n",
      "[[7. 8.]\n",
      " [5. 0.]\n",
      " [2. 7.]]\n",
      "--\n",
      "c=\n",
      "[[7.         8.        ]\n",
      " [6.         4.        ]\n",
      " [4.66666667 5.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a lower triangular matrix\n",
    "a = tf.linalg.band_part(tf.ones((3, 3), dtype=tf.float64), -1, 0)\n",
    "a = a / tf.reduce_sum(a, axis=1, keepdims=True)\n",
    "\n",
    "# Create a random matrix with integer values between 0 and 10\n",
    "b = tf.constant(np.random.randint(0, 10, size=(3, 2)).astype(np.float64))\n",
    "\n",
    "# Perform matrix multiplication\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "# Print the results\n",
    "print('a=')\n",
    "print(a.numpy())\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b.numpy())\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8df91",
   "metadata": {},
   "source": [
    "#### Version 2 (Reproducing xbow via matrix mulitplication instead of loop multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6c8afc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.5  , 0.5  , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.333, 0.333, 0.333, 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.25 , 0.25 , 0.25 , 0.25 , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.2  , 0.2  , 0.2  , 0.2  , 0.2  , 0.   , 0.   , 0.   ],\n",
       "       [0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.   , 0.   ],\n",
       "       [0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.   ],\n",
       "       [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float64), -1, 0) \n",
    "weights = weights / tf.reduce_sum(weights, axis=1, keepdims=True)\n",
    "\n",
    "# Round each value to 3 decimal points for better visibility\n",
    "weights_numpy_rounded = np.round(weights.numpy().tolist(), decimals=3)\n",
    "weights_numpy_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f2c1ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = tf.matmul(weights, x) # Here (B,T,T) x (B,T,C) = (B,T,C).   where B in (B,T,T) was automatically created by tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "710daaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow =  [[ 0.95310103 -0.33585035]\n",
      " [ 0.02886359 -0.21847474]\n",
      " [-0.34324746 -0.22362592]\n",
      " [-0.11893325 -0.93038263]\n",
      " [-0.18926548 -1.11807985]\n",
      " [ 0.18329706 -1.00265691]\n",
      " [-0.09440874 -0.79740459]\n",
      " [-0.28983498 -0.61012586]]\n",
      "\n",
      "Same as\n",
      "\n",
      "xbow2 =  [[ 0.95310103 -0.33585035]\n",
      " [ 0.02886359 -0.21847474]\n",
      " [-0.34324746 -0.22362592]\n",
      " [-0.11893325 -0.93038263]\n",
      " [-0.18926548 -1.11807985]\n",
      " [ 0.18329706 -1.00265691]\n",
      " [-0.09440874 -0.79740459]\n",
      " [-0.28983498 -0.61012586]]\n"
     ]
    }
   ],
   "source": [
    "print('xbow = ',xbow[0].numpy())\n",
    "print('\\nSame as\\n')\n",
    "print('xbow2 = ',xbow2[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97027d",
   "metadata": {},
   "source": [
    "#### Version 3 (Using softmax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dc250f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lower triangular matrix and initialize wei with zeros\n",
    "tril = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float64), -1, 0)\n",
    "weights = tf.zeros((T, T), dtype=tf.float64)\n",
    "\n",
    "# Mask the upper triangular part with -inf\n",
    "weights = tf.where(tf.equal(tril, 0), float('-inf'), weights)\n",
    "\n",
    "# Apply softmax along the last dimension\n",
    "weights = tf.nn.softmax(weights, axis=-1)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "xbow3 = tf.matmul(weights, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a4ab247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow =  [[ 0.95310103 -0.33585035]\n",
      " [ 0.02886359 -0.21847474]\n",
      " [-0.34324746 -0.22362592]\n",
      " [-0.11893325 -0.93038263]\n",
      " [-0.18926548 -1.11807985]\n",
      " [ 0.18329706 -1.00265691]\n",
      " [-0.09440874 -0.79740459]\n",
      " [-0.28983498 -0.61012586]]\n",
      "\n",
      "Same as\n",
      "\n",
      "xbow2 =  [[ 0.95310103 -0.33585035]\n",
      " [ 0.02886359 -0.21847474]\n",
      " [-0.34324746 -0.22362592]\n",
      " [-0.11893325 -0.93038263]\n",
      " [-0.18926548 -1.11807985]\n",
      " [ 0.18329706 -1.00265691]\n",
      " [-0.09440874 -0.79740459]\n",
      " [-0.28983498 -0.61012586]]\n"
     ]
    }
   ],
   "source": [
    "print('xbow = ',xbow[0].numpy())\n",
    "print('\\nSame as\\n')\n",
    "print('xbow2 = ',xbow3[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205644b5",
   "metadata": {},
   "source": [
    "### 3.3 Self-attention mechanism context (better method)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be8600",
   "metadata": {},
   "source": [
    "#### Version 4 (Using Key, Query, Value upon version 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6361e856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 32])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = tf.random.normal((B, T, C))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7e509500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape :  (4, 8, 32)\n",
      "k.shape :  (4, 8, 16)\n",
      "q.shape :  (4, 8, 16)\n",
      "wei.shape :  (4, 8, 8)\n",
      "(4, 8, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "\n",
    "key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "\n",
    "print('x.shape : ', x.shape)\n",
    "k = key(x)   # (B, T, 32) => (B, T, 16)\n",
    "q = query(x) # (B, T, 32) => (B, T, 16)\n",
    "print('k.shape : ', k.shape)\n",
    "print('q.shape : ', q.shape)\n",
    "\n",
    "wei = tf.matmul(q, tf.transpose(k, perm=[0, 2, 1])* head_size**-0.5)  # (B, T, T)  #also scaling after transpose, check below to see why\n",
    "\n",
    "print('wei.shape : ',wei.shape)\n",
    "\n",
    "tril = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float64), -1, 0)\n",
    "wei = tf.where(tf.equal(tril, 0), float('-inf'), wei)\n",
    "\n",
    "# Apply softmax along the last dimension\n",
    "wei = tf.nn.softmax(wei, axis=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = tf.matmul(wei, v)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed262c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba1841d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.55890644, 0.44109353, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.36212704, 0.3926684 , 0.24520455, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.17977196, 0.32847154, 0.08661044, 0.40514615, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.2580921 , 0.03877385, 0.26799572, 0.25024772, 0.18489058,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.15109922, 0.45920232, 0.09098168, 0.07991469, 0.03213087,\n",
       "        0.1866712 , 0.        , 0.        ],\n",
       "       [0.2580665 , 0.03653046, 0.13164522, 0.01842463, 0.08224345,\n",
       "        0.45676315, 0.01632656, 0.        ],\n",
       "       [0.03404133, 0.00912395, 0.03316646, 0.1516977 , 0.09125233,\n",
       "        0.00497304, 0.0166554 , 0.65908974]], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "08f9b86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1924978 , 0.14260589, 0.23511736, 0.14260589, 0.287173  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without scaling \n",
    "tf.nn.softmax(tf.constant([0.1, -0.2, 0.3, -0.2, 0.5]), axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "40815e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03260834, 0.00295816, 0.1615102 , 0.00295816, 0.79996514],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without scaling the softmax favours larger values big time as visible\n",
    "tf.nn.softmax(tf.constant([0.1, -0.2, 0.3, -0.2, 0.5])*8, axis=-1).numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d7e30",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0522a",
   "metadata": {},
   "source": [
    "## 4 Bigram Language Model (With Transformer Block) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c8726",
   "metadata": {},
   "source": [
    "### 4.1 Transformer Head Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "64b72e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super(Head, self).__init__()\n",
    "        \n",
    "        self.key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.tril = tf.linalg.band_part(tf.ones((block_size, block_size), dtype=tf.float64), -1, 0)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        k = key(x)   # (B, T, C) => (B, T, head_size)\n",
    "        q = query(x) # (B, T, C) => (B, T, head_size)\n",
    "        \n",
    "        #Compute Attention Score (Affinities)\n",
    "        wei = tf.matmul(q, tf.transpose(k, perm=[0, 2, 1])* C**-0.5)  # (B,T,C) @ (B, C, T) => (B, T, T)\n",
    "        \n",
    "        wei = tf.where(tf.equal(self.tril[:T,:T], 0), float('-inf'), wei) #Here we cut of future context which makes this head a decoder block\n",
    "\n",
    "        # Apply softmax along the last dimension\n",
    "        wei = tf.nn.softmax(wei, axis=-1)\n",
    "        #Perform Weighted Aggregation of the values\n",
    "        v = value(x) # (B,T,C)\n",
    "        out = tf.matmul(wei, v) #(B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0103f",
   "metadata": {},
   "source": [
    "### 4.2 Bigram Language Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "787b18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        \n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embd)\n",
    "        self.sa_head = tf.keras.layers.Dense(n_embd)\n",
    "        self.lm_head = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None, training=False):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emd = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emd = self.position_embedding_table(tf.range(T)) #(T, C)\n",
    "        x = tok_emd + pos_emd #(B,T,C) # Holds Token identities & position\n",
    "        x = self.sa_head(x) #Apply one head of self attention. (B,T,C) \n",
    "        logits = self.lm_head(x) # (B, T, vocab_size) # finally decoded here\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = tf.reshape(logits, (B*T, C))\n",
    "            targets = tf.reshape(targets, (B*T,))\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] #Because if idx is more than block size then our positional embd above\n",
    "                                            #is gonna run out of scope because it has embeddings of upto block size\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), 1)  # (B, 1)\n",
    "            idx = tf.concat([idx, idx_next], axis=1)  # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8f8edab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65)\n",
      "tf.Tensor(4.169185, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size, n_embd)\n",
    "\n",
    "\n",
    "logits, loss = model(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9215541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZiQlH\n",
      "XYbrOU&py,dn-ovbsESOAqBKsk:yxV'-lNES!OU$3?xB,CpHqR\n",
      "\n",
      "i;TFC,MjPQ3FbQ,SjCJ.hetJ.Mv!WQ?q,&A.M KHBO\n"
     ]
    }
   ],
   "source": [
    "print(''.join(decode(model.generate(idx=tf.zeros((1, 1), dtype=tf.int64), max_new_tokens=100).numpy()[0].tolist())))\n",
    "\n",
    "#Total Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "70b21af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  2.4673042\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 10000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Sample a batch of data\n",
    "    xbatch, ybatch = get_batch('train')  # Assuming you have a function get_batch\n",
    "\n",
    "    # Evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xbatch, ybatch)\n",
    "\n",
    "    # Compute gradients and update weights\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "print('Loss = ', loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cdfeb5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A or INGLOrle bere inmard meande LAnghall ysatan\n",
      "Tousave foistha avende INGllit the wil acomear\n",
      "Feder re yoom rne lem! whe my I hith tis DUCo ixpeparig:\n",
      "\n",
      "CHour beetay pathe Anen. mak\n",
      "HAs an P qu ben y ghe,\n",
      "LENorolt fthad n I eathe thet Th g:\n",
      "A'sourssid h t IXEY:\n",
      "I:\n",
      "\n",
      "ARYorve oll?\n",
      "S:\n",
      "Wisef s outleveryf Whond, d asive ntss s llamuine,\n",
      "T: ifokes besuce d hatorul n suces he se rt lin he aumut?\n",
      "\n",
      "I ok? s ee.\n",
      "ARee y V:\n",
      "I t ware f aves.\n",
      "\n",
      "D:\n",
      "Serssor'd undpirthouse ger lve lyore.\n",
      "He tld Maldig ore sth Sean\n"
     ]
    }
   ],
   "source": [
    "#Somewhat stuctured results\n",
    "\n",
    "print(''.join(decode(model.generate(idx=tf.zeros((1, 1), dtype=tf.int64), max_new_tokens=500).numpy()[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ca779",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
